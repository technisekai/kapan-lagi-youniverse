# Core function for database
import psycopg2
from typing import Literal
from psycopg2.extras import execute_values, Json
from datetime import datetime

def postgresql_connection(host: str, port: int, user: str, password: str, database: str):
    connection = psycopg2.connect(
        host=host,
        port=port,
        user=user,
        password=password,
        database=database
    )
    return connection

def execute_query(
    connection: postgresql_connection, 
    query: str
):
    cursor = connection.cursor()
    cursor.execute(query)
    connection.commit()

def dict_to_postgresql_schema(schema: dict):
    postgresql_schema = {}
    # Mapping dict data type to postgre data type
    for key in schema.keys():
        json_type_name = (type(schema[key]).__name__).lower()
        if json_type_name in ['str', 'nonetype']:
            postgresql_schema[key] = 'varchar'
        elif json_type_name in ['int', 'float', 'decimal']:
            postgresql_schema[key] = 'numeric'
        elif json_type_name in ['list', 'dict']:
            postgresql_schema[key] = 'jsonb'
        elif json_type_name == 'bool':
            postgresql_schema[key] = 'boolean'
        elif json_type_name == 'date':
            postgresql_schema[key] = 'date'
        elif json_type_name in ['datetime', 'timestamp']:
            postgresql_schema[key] = 'timestamp'
        else:
            raise Exception(f'Err {json_type_name} data type not defined!')
    return postgresql_schema

def create_table(connection, schema_name: str, table_name: str, json_schema: dict):
    table_schema = dict_to_postgresql_schema(json_schema)
    cursor = connection.cursor()
    query_create_table = f"""
        create table if not exists {schema_name}.{table_name} (
            _id int8 not null generated by default as identity,
            {', '.join([' '.join([f'"{x[0]}"', x[1]]) for x in zip(table_schema.keys(), table_schema.values())])},
            _hashrow varchar,
            _created_at timestamp null default current_timestamp,
            _updated_at timestamp null default current_timestamp
        )
    """
    cursor.execute(query_create_table)
    connection.commit()
    cursor.close()

def insert_data_batch(
    connection,
    data: list[dict], 
    schema_name: str,
    table_name: str,
    table_schema: dict,
    unique_key_name: str,
    hashrow_cols: list = None,
    chunksize: int = 1000
):
    table_schema['_hashrow'] = 'varchar'
    cursor = connection.cursor()
    for idx in range(0, len(data), chunksize):
        columns = [f'"{x}"' for x in data[idx].keys()]
        values = [[Json(row.get(col, None)) if isinstance(row.get(col, None), (dict, list)) \
                else row.get(col, None) \
                    for col in data[idx].keys()] for row in data[idx:idx+chunksize]]
        hashrow_cols = columns if not hashrow_cols else hashrow_cols
        values_flatten = [item for value in values for item in value]
        
        cols_cnt_values = f"({', '.join(['%s'] * len(values[0]))})"
        rows_cnt_values = f"{', '.join([cols_cnt_values] * len(values))}"
        query_update_data = f"""
            with records_chunk as (
                select distinct
                    {', '.join([str(f'''{x}::{table_schema[x.replace('"', '')]}''') for x in columns])},
                    md5(
                        concat_ws (
                            '|',
                            {', '.join([str(f'''{x}::{table_schema[x.replace('"', '')]}''') for x in columns])}
                        )
                    ) as _hashrow
                from (
                    select * from (values {rows_cnt_values}) as t({', '.join(columns)})
                )
            )
            merge into {schema_name}.{table_name} old
            using records_chunk new
            on old."{unique_key_name}"::{table_schema[unique_key_name]} = new."{unique_key_name}"
            when not matched then
                insert ({','.join(columns)}, _hashrow)
                values ({','.join([f'new.{x}' for x in columns])}, new._hashrow)
        """
        cursor.execute(query_update_data, values_flatten)
        connection.commit()